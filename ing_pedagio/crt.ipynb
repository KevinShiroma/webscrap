{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extrair_tabelas(url):\n",
    "    # Faz a requisição para obter o conteúdo da página\n",
    "    resposta = requests.get(url)\n",
    "\n",
    "    if resposta.status_code == 200:\n",
    "        # Cria um objeto BeautifulSoup para parsear o HTML\n",
    "        soup = BeautifulSoup(resposta.content, 'html.parser')\n",
    "        \n",
    "        # Inicializa uma lista para armazenar todos os DataFrames\n",
    "        dataframes = []\n",
    "\n",
    "        # Busca todas as tabelas na página\n",
    "        tables = soup.find_all('table')\n",
    "        \n",
    "        # Itera sobre todas as tabelas encontradas e as converte em DataFrames\n",
    "        for i, table in enumerate(tables):\n",
    "            try:\n",
    "                # Converte o conteúdo da tabela para StringIO antes de usar read_html\n",
    "                table_str = StringIO(str(table))\n",
    "                df = pd.read_html(table_str)[0]\n",
    "                dataframes.append(df)\n",
    "                print(f\"Tabela {i+1}:\\n{df}\\n\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Erro ao processar a tabela {i+1}: {e}\")\n",
    "        \n",
    "        return dataframes\n",
    "    else:\n",
    "        print(f\"Falha ao acessar a página: {resposta.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_tabela(url):\n",
    "    # Faz a requisição para obter o conteúdo da página\n",
    "    resposta = requests.get(url)\n",
    "\n",
    "    if resposta.status_code == 200:\n",
    "        # Cria um objeto BeautifulSoup para parsear o HTML\n",
    "        soup = BeautifulSoup(resposta.text, 'html.parser')\n",
    "\n",
    "        # Encontrando a div com id 'parent-fieldname-text'\n",
    "        div_content = soup.find('div', id='parent-fieldname-text')\n",
    "\n",
    "        if div_content:\n",
    "            # Encontra a primeira tabela na página\n",
    "            tabela = soup.find_all(\"table\")\n",
    "            print(tabela)\n",
    "\n",
    "            # Extrai as linhas da tabela\n",
    "            linhas = tabela.find_all(\"tr\")\n",
    "            \n",
    "            # Inicializa uma lista para armazenar os dados\n",
    "            dados = []\n",
    "            \n",
    "            # Itera sobre as linhas da tabela\n",
    "            for linha in linhas:\n",
    "                # Extrai as células de cada linha\n",
    "                celulas = linha.find_all(\"td\")\n",
    "                # Extrai o texto de cada célula e adiciona à lista 'dados'\n",
    "                dados.append([celula.get_text(strip=True) for celula in celulas])\n",
    "            \n",
    "            # Remove linhas vazias\n",
    "            dados = [linha for linha in dados if any(linha)]\n",
    "\n",
    "            # Criar o DataFrame com os dados extraídos\n",
    "            tabela_autopista = pd.DataFrame(dados)\n",
    "            \n",
    "            \n",
    "            return tabela_autopista\n",
    "        else:\n",
    "            print(\"Tabela não encontrada na página.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Falha ao acessar a página: {resposta.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz a requisição para obter o conteúdo da página\n",
    "url = 'https://www.gov.br/antt/pt-br/assuntos/rodovias/concessionarias/lista-de-concessoes/contratos-encerrados/crt/tarifas-de-pedagio'\n",
    "resposta = requests.get(url)\n",
    "\n",
    "if resposta.status_code == 200:\n",
    "        # Cria um objeto BeautifulSoup para parsear o HTML\n",
    "        soup = BeautifulSoup(resposta.text, 'html.parser')\n",
    "\n",
    "        # Encontrando a div com id 'parent-fieldname-text'\n",
    "        div_content = soup.find('div', id='parent-fieldname-text')\n",
    "\n",
    "        if div_content:\n",
    "            # Encontra a primeira tabela na página\n",
    "            tabela = soup.find_all(\"table\")\n",
    "\n",
    "\n",
    "            # Extrai as linhas da tabela\n",
    "            linhas = tabela.find_all(\"tr\")\n",
    "            print(linhas)\n",
    "            \n",
    "            # Inicializa uma lista para armazenar os dados\n",
    "            dados = []\n",
    "            \n",
    "            # Itera sobre as linhas da tabela\n",
    "            for linha in linhas:\n",
    "                # Extrai as células de cada linha\n",
    "                celulas = linha.find_all(\"td\")\n",
    "                # Extrai o texto de cada célula e adiciona à lista 'dados'\n",
    "                dados.append([celula.get_text(strip=True) for celula in celulas])\n",
    "            \n",
    "            # Remove linhas vazias\n",
    "            dados = [linha for linha in dados if any(linha)]\n",
    "\n",
    "            # Criar o DataFrame com os dados extraídos\n",
    "            tabela_autopista = pd.DataFrame(dados)\n",
    "            \n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(\"Tabela não encontrada na página.\")\n",
    "            \n",
    "else:\n",
    "        print(f\"Falha ao acessar a página: {resposta.status_code}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m url_valores \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.gov.br/antt/pt-br/assuntos/rodovias/concessionarias/lista-de-concessoes/contratos-encerrados/crt/tarifas-de-pedagio\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m tabela_autopista \u001b[38;5;241m=\u001b[39m \u001b[43mextrair_tabela\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_valores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m display(tabela_autopista)\n",
      "Cell \u001b[1;32mIn[51], line 19\u001b[0m, in \u001b[0;36mextrair_tabela\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     16\u001b[0m tabela \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Extrai as linhas da tabela\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m linhas \u001b[38;5;241m=\u001b[39m \u001b[43mtabela\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Inicializa uma lista para armazenar os dados\u001b[39;00m\n\u001b[0;32m     22\u001b[0m dados \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\kevin.shiroma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\bs4\\element.py:2433\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   2432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   2434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultSet object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key\n\u001b[0;32m   2435\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "url_valores = \"https://www.gov.br/antt/pt-br/assuntos/rodovias/concessionarias/lista-de-concessoes/contratos-encerrados/crt/tarifas-de-pedagio\"\n",
    "tabela_autopista = extrair_tabela(url_valores)\n",
    "display(tabela_autopista)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
